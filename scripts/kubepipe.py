from kfp import dsl
from kfp import compiler

# –¢–≤–æ–π –æ–±—Ä–∞–∑ —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–∞–º–∏ (Feast, MLflow, Sklearn, Boto3)
BASE_IMAGE = "feast-trainer:v3" 

# --- –ö–û–ú–ü–û–ù–ï–ù–¢ 1: ETL & Feast Sync ---
# –û–±—ä–µ–¥–∏–Ω–∏–º –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–ª–∏–≤–∫—É –≤ Redis, —á—Ç–æ–±—ã –Ω–µ –≥–æ–Ω—è—Ç—å –¥–∞–Ω–Ω—ã–µ –º–µ–∂–¥—É –ø–æ–¥–∞–º–∏
@dsl.component(base_image=BASE_IMAGE)
def etl_and_feast_op(
    minio_url: str,
    redis_url: str,
    access_key: str,
    secret_key: str
):
    import os
    import pandas as pd
    import numpy as np
    from sklearn.datasets import load_iris
    import boto3
    from io import BytesIO
    from datetime import datetime
    
    # 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    os.environ["AWS_ENDPOINT_URL"] = minio_url
    os.environ["AWS_ACCESS_KEY_ID"] = access_key
    os.environ["AWS_SECRET_ACCESS_KEY"] = secret_key
    os.environ["AWS_REGION"] = "us-east-1"
    
    print("--- [Step 1] Generating Data ---")
    iris = load_iris()
    df = pd.DataFrame(iris.data, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])
    df.columns = [c.replace(' ', '_') for c in df.columns]
    df['target'] = iris.target
    df['flower_id'] = np.arange(len(df))
    df['event_timestamp'] = pd.Timestamp.now()

    # 2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ MinIO (Offline Store)
    s3 = boto3.client('s3', endpoint_url=minio_url, aws_access_key_id=access_key, aws_secret_access_key=secret_key)
    try:
        s3.create_bucket(Bucket="feast-data")
    except Exception:
        pass

    out_buffer = BytesIO()
    df.to_parquet(out_buffer, index=False)
    s3.put_object(Bucket="feast-data", Key="iris.parquet", Body=out_buffer.getvalue())
    print("‚úÖ Data saved to MinIO")

    # 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Feast "–Ω–∞ –ª–µ—Ç—É"
    # –ú—ã –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∞–¥—Ä–µ—Å–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞
    repo_path = "/app/feature_repo"
    yaml_content = f"""
project: iris_project
registry: s3://feast-data/registry.db
provider: local
online_store:
    type: redis
    connection_string: "{redis_url}"
offline_store:
    type: file
"""
    with open(f"{repo_path}/feature_store.yaml", "w") as f:
        f.write(yaml_content)
    
    os.chdir(repo_path)
    
    # 4. Feast Apply & Materialize
    print("--- [Step 2] Syncing to Redis ---")
    if os.system("feast apply") != 0: raise RuntimeError("Feast Apply Failed")
    
    # –ú–∞—Ç–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ (–∏–∑ S3 –≤ Redis)
    if os.system(f"feast materialize-incremental {datetime.now().isoformat()}") != 0: 
        raise RuntimeError("Materialize Failed")
        
    print("‚úÖ Feast synced successfully")


# --- –ö–û–ú–ü–û–ù–ï–ù–¢ 2: –û–±—É—á–µ–Ω–∏–µ ---
@dsl.component(base_image=BASE_IMAGE)
def train_op(
    minio_url: str,
    mlflow_url: str,
    access_key: str,
    secret_key: str
) -> str: # –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç URI –º–æ–¥–µ–ª–∏ (s3://...)
    import os
    import pandas as pd
    import numpy as np
    import mlflow
    import mlflow.sklearn
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score
    from mlflow.models.signature import infer_signature
    from feast import FeatureStore
    
    # Env setup
    os.environ["AWS_ENDPOINT_URL"] = minio_url
    os.environ["AWS_ACCESS_KEY_ID"] = access_key
    os.environ["AWS_SECRET_ACCESS_KEY"] = secret_key
    os.environ["MLFLOW_S3_ENDPOINT_URL"] = minio_url
    os.environ["MLFLOW_S3_IGNORE_TLS"] = "true"
    os.environ["AWS_REGION"] = "us-east-1"
    os.system("pip install scikit-learn==1.3.2 mlflow==2.8.1 'pydantic<2.0.0' numpy<2.0.0")
    # Feast Config (Offline only)
    repo_path = "/app/feature_repo"
    yaml_content = f"""
project: iris_project
registry: s3://feast-data/registry.db
provider: local
offline_store:
    type: file
"""
    with open(f"{repo_path}/feature_store.yaml", "w") as f:
        f.write(yaml_content)
        
    fs = FeatureStore(repo_path=repo_path)
    
    # 1. –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ Feast
    print("--- [Step 3] Fetching Training Data ---")
    entity_df = pd.DataFrame.from_dict({
        "flower_id": np.arange(150),
        "event_timestamp": [pd.Timestamp.now()] * 150
    })
    
    df = fs.get_historical_features(
        entity_df=entity_df,
        features=[
            "iris_stats:sepal_length",
            "iris_stats:sepal_width",
            "iris_stats:petal_length",
            "iris_stats:petal_width",
            "iris_stats:target",
        ],
    ).to_df().dropna()
    
    X = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].astype(np.float32)
    y = df['target']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # 2. MLflow Training
    print("--- [Step 4] Training Model ---")
    mlflow.set_tracking_uri(mlflow_url)
    mlflow.set_experiment("kserve-experiment")
    
    model_name = "IrisKServeModel"
    with mlflow.start_run() as run:
        clf = RandomForestClassifier(n_estimators=50)
        clf.fit(X_train, y_train)
        
        acc = accuracy_score(y_test, clf.predict(X_test))
        mlflow.log_metric("accuracy", acc)
        
        signature = infer_signature(X_train, clf.predict(X_train))
        mlflow.sklearn.log_model(clf, artifact_path="model", registered_model_name=model_name, signature=signature)
        
        # –ü–æ–ª—É—á–∞–µ–º URI –º–æ–¥–µ–ª–∏ (s3://mlflow/...)
        model_uri = mlflow.get_artifact_uri("model")
        print(f"‚úÖ Model saved at {model_uri}")
        return model_uri

# --- –ö–û–ú–ü–û–ù–ï–ù–¢ 3: KServe Deploy ---
@dsl.component(
    base_image="python:3.9", # –¢—É—Ç –º–æ–∂–Ω–æ –ª–µ–≥–∫–∏–π –æ–±—Ä–∞–∑, –≥–ª–∞–≤–Ω–æ–µ kubernetes lib
    packages_to_install=["kubernetes"]
)
def kserve_deploy_op(
    model_uri: str,
    model_name: str = "iris-classifier",
    namespace: str = "kubeflow"
):
    from kubernetes import client, config
    import json
    
    print(f"üöÄ Deploying {model_uri} to KServe...")
    
    config.load_incluster_config()
    api = client.CustomObjectsApi()
    
    # –ú–∞–Ω–∏—Ñ–µ—Å—Ç InferenceService (–∞–Ω–∞–ª–æ–≥ Seldon Model)
    isvc = {
        "apiVersion": "serving.kserve.io/v1beta1",
        "kind": "InferenceService",
        "metadata": {
            "name": model_name,
            "namespace": namespace,
            "annotations": {
                "sidecar.istio.io/inject": "false" # –ò–Ω–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –æ—Ç–∫–ª—é—á–∞—Ç—å, –µ—Å–ª–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã
            }
        },
        "spec": {
            "predictor": {
                "serviceAccountName": "kserve-sa", # –ù–∞—à SA —Å –¥–æ—Å—Ç—É–ø–æ–º –∫ MinIO
                "model": {
                    "modelFormat": {
                        "name": "sklearn" # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π sklearn —Å–µ—Ä–≤–µ—Ä KServe
                    },
                    "storageUri": model_uri # s3://mlflow/...
                }
            }
        }
    }
    
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∑–¥–∞—Ç—å
        api.create_namespaced_custom_object(
            group="serving.kserve.io",
            version="v1beta1",
            namespace=namespace,
            plural="inferenceservices",
            body=isvc
        )
        print("‚úÖ InferenceService created!")
    except client.exceptions.ApiException as e:
        if e.status == 409: # –£–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç -> –æ–±–Ω–æ–≤–ª—è–µ–º
            print("üîÑ InferenceService exists, patching...")
            # –ü–æ–ª—É—á–∞–µ–º resourceVersion –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –ø–∞—Ç—á–∞ (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º merge-patch)
            existing = api.get_namespaced_custom_object(
                group="serving.kserve.io",
                version="v1beta1",
                namespace=namespace,
                plural="inferenceservices",
                name=model_name
            )
            isvc["metadata"]["resourceVersion"] = existing["metadata"]["resourceVersion"]
            
            api.replace_namespaced_custom_object(
                group="serving.kserve.io",
                version="v1beta1",
                namespace=namespace,
                plural="inferenceservices",
                name=model_name,
                body=isvc
            )
            print("‚úÖ InferenceService updated!")
        else:
            raise e

# --- –ü–ê–ô–ü–õ–ê–ô–ù ---
@dsl.pipeline(
    name='kserve-mlops-pipeline',
    description='Feast -> MLflow -> KServe'
)
def kserve_pipeline():
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–≤ –∏–¥–µ–∞–ª–µ - —á–µ—Ä–µ–∑ Secrets)
    minio = "http://minio-service.kubeflow.svc.cluster.local:9000"
    mlflow = "http://mlflow.kubeflow.svc.cluster.local:5000"
    redis = "redis-master.kubeflow.svc.cluster.local:6379"
    
    # 1. –î–∞–Ω–Ω—ã–µ
    etl_task = etl_and_feast_op(
        minio_url=minio,
        redis_url=redis,
        access_key="minio",
        secret_key="minio123"
    )
    
    # 2. –û–±—É—á–µ–Ω–∏–µ (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç ETL, —Ö–æ—Ç—è KFP –º–æ–∂–µ—Ç —Å–∞–º –ø–æ–Ω—è—Ç—å, –µ—Å–ª–∏ –±—ã –º—ã –ø–µ—Ä–µ–¥–∞–≤–∞–ª–∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã)
    train_task = train_op(
        minio_url=minio,
        mlflow_url=mlflow,
        access_key="minio",
        secret_key="minio123"
    )
    train_task.after(etl_task)
    
    # 3. –î–µ–ø–ª–æ–π
    deploy_task = kserve_deploy_op(
        model_uri=train_task.output,
        model_name="iris-model",
        namespace="kubeflow"
    )

if __name__ == '__main__':
    compiler.Compiler().compile(
        pipeline_func=kserve_pipeline,
        package_path='kserve_pipeline.yaml'
    )
    print("‚úÖ Compiled!")
